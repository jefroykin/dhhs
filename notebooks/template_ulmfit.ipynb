{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os\n",
    "import re\n",
    "import requests\n",
    "import string\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import emoji\n",
    "from emoji.unicode_codes import UNICODE_EMOJI\n",
    "import itertools\n",
    "import multiprocessing\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, precision_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import fastai\n",
    "from fastai import *\n",
    "from fastai.text import * \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from functools import partial\n",
    "import io\n",
    "import os\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import EnglishStemmer  # import the French stemming library\n",
    "\n",
    "VOWELS = [\"a\", \"e\", \"i\", \"o\", \"u\", \"y\"]\n",
    "stemmer = EnglishStemmer()\n",
    "\n",
    "DFS_PATH = '../data/training_set.csv'\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stopwords_list(language):\n",
    "    return stopwords.words(str(language))\n",
    "\n",
    "\n",
    "def filter_sentence_stopwords(tokens, list_stopwords):\n",
    "\n",
    "    clean_tokens = [w for w in tokens if w not in list_stopwords and len(w) > 1\n",
    "                    and w.isalpha()]\n",
    "    return clean_tokens\n",
    "\n",
    "\n",
    "def lowercase(text):\n",
    "    s = []\n",
    "    words = text.split(' ')\n",
    "    for word in words:\n",
    "        if word.isupper():\n",
    "            s.append(word.lower())\n",
    "        else:\n",
    "            s.append(word)\n",
    "\n",
    "    return ' '.join(s)\n",
    "\n",
    "\n",
    "def clean_ponctuation(text):\n",
    "    exclude = set((string.punctuation + '«»').replace('.', '').replace('!', '')\n",
    "                  .replace('?', ''))\n",
    "    s = ''.join(ch for ch in text if ch not in exclude)\n",
    "    return s\n",
    "\n",
    "\n",
    "def clean_http(text):\n",
    "    return re.sub(r'http\\S+', '', text)\n",
    "\n",
    "\n",
    "def cleaning_words(text):\n",
    "    s = []\n",
    "    words = text.split(' ')\n",
    "    for word in words:\n",
    "        if word.isupper():\n",
    "            s.append(word.lower())\n",
    "        elif word.islower():\n",
    "            s.append(word)\n",
    "        else:\n",
    "            for word in re.findall('[A-Z][^A-Z]*', word):\n",
    "                s.append(word)\n",
    "\n",
    "    return ' '.join(s)\n",
    "\n",
    "\n",
    "def text_cleaning(text):\n",
    "    text_cleaned_http = clean_http(text)\n",
    "    text_cleaned_ponctuation = clean_ponctuation(text_cleaned_http)\n",
    "    text_cleaned = cleaning_words(text_cleaned_ponctuation)\n",
    "\n",
    "    return text_cleaned\n",
    "\n",
    "\n",
    "def word_tokenizer(post):\n",
    "    return nltk.word_tokenize(post.lower())\n",
    "\n",
    "\n",
    "def stemmer_post(tokens):\n",
    "    stemmer_word = []\n",
    "    for w in tokens:\n",
    "        stemmer_word.append(stemmer.stem(w))\n",
    "    return stemmer_word\n",
    "\n",
    "\n",
    "def _find_emojis(message):\n",
    "    emoji_list = []\n",
    "    for word in message:\n",
    "        if word in emoji.UNICODE_EMOJI:\n",
    "            emoji_list.append(word)\n",
    "    return emoji_list\n",
    "\n",
    "\n",
    "def _erase_emojis(message, emojis_list):\n",
    "    for emo in emojis_list:\n",
    "        message = message.replace(emo, '')\n",
    "    return message\n",
    "\n",
    "\n",
    "def _add_emojis_tokens(tokens_list, emojis_list):\n",
    "    return emojis_list + tokens_list\n",
    "\n",
    "\n",
    "def emoji_to_text(s):\n",
    "    res = []\n",
    "    for e in s:\n",
    "        r = UNICODE_EMOJI[e].replace(':','')\n",
    "        try:\n",
    "            r = r.split('_')\n",
    "            res.append(r)\n",
    "        except:\n",
    "            res.append([r])\n",
    "    try:\n",
    "        return [item for sublist in res for item in sublist]\n",
    "    except:\n",
    "        return ['']\n",
    "    \n",
    "def text_to_tokens(text):\n",
    "    try:\n",
    "        text = text.replace('❤','love')\n",
    "    except:\n",
    "        pass\n",
    "    emojis_list = _find_emojis(text)\n",
    "    emojis_list = emoji_to_text(emojis_list)\n",
    "    wes = _erase_emojis(text, emojis_list)\n",
    "    wes = text_cleaning(wes)\n",
    "    stopwords = stopwords_list('english') #OUT\n",
    "    wes = wes.lower().split(' ')\n",
    "    wes = filter_sentence_stopwords(wes, stopwords)\n",
    "    wes = stemmer_post(wes)\n",
    "\n",
    "    return _add_emojis_tokens(wes, emojis_list)\n",
    "\n",
    "def processing_post(post):\n",
    "    tokens = text_to_tokens(post)\n",
    "\n",
    "    cleaned_post = ' '.join(tokens)\n",
    "    return cleaned_post\n",
    "\n",
    "def processing_corpus(corpus):\n",
    "    cleaned_corpus = [processing_post(post) for post in corpus]\n",
    "    corpus = list(cleaned_corpus)\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Datas Set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(DFS_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['content'] = processing_corpus(data['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Train/Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data['content'].values, \n",
    "                                                    data['label'].values, \n",
    "                                                    test_size=0.1, \n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UlmFit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = pd.concat([X_train, y_train], axis=1).reset_index(drop=True)\n",
    "validating = pd.concat([X_val, y_val], axis=1).reset_index(drop=True)\n",
    "\n",
    "training.columns = ['text', 'label']\n",
    "validating.columns = ['text', 'label']\n",
    "\n",
    "validating = validating.reindex(sorted(validating.columns), axis=1)\n",
    "training = training.reindex(sorted(training.columns), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Language model data\n",
    "data_lm = TextLMDataBunch.from_df(train_df = training, valid_df = validating, path = \"\")\n",
    "\n",
    "# Classifier model data\n",
    "data_clas = TextClasDataBunch.from_df(path = \"\", \n",
    "                                      train_df = training, \n",
    "                                      valid_df = validating, \n",
    "                                      vocab=data_lm.train_ds.vocab,\n",
    "                                      bs=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit UlmFit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = language_model_learner(data_lm, pretrained_model=URLs.WT103, drop_mult=0.5)\n",
    "learn.fit_one_cycle(10, 1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save_encoder('ft_enc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine tuning with our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = text_classifier_learner(data_clas, drop_mult=0.7)\n",
    "learn.load_encoder('ft_enc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(25, 1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions and results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds, targets = learn.get_preds()\n",
    "predictions = np.argmax(preds, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(y_test, predictions, average=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_score(y_test, predictions, average=None) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_test, predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Zencity_env",
   "language": "python",
   "name": "zencity_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
